{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возможно понадобится\n",
    "from IPython.display import clear_output\n",
    "!pip install devito\n",
    "clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b366fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# import devito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd03f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CARDS = 3\n",
    "NUM_DETECTORS = 60\n",
    "NUM_TIMESTEPS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "\n",
    "graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172a095",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_element(xs, y):\n",
    "    fig, ax = plt.subplots(1, xs.shape[0], figsize=(25, 5))\n",
    "    for i in range(xs.shape[0]):\n",
    "        buff = xs[i].T\n",
    "        if np.any(buff):\n",
    "            for j in range(buff.shape[1]):\n",
    "                buff[:, j] = buff[:, j]/np.max(np.abs(buff[:, j]))\n",
    "        ax[i].imshow(buff, cmap='gray', vmin=-0.05, vmax=0.05)\n",
    "        ax[i].axis('off')\n",
    "        ax[i].set_aspect('auto')\n",
    "    ax[xs.shape[0]//2].plot(y, c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d8fff",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32405660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, num_cards, num_detectors, num_timesteps, path, path_noise=None,\n",
    "                 min_noise_lvl=0.2,\n",
    "                 max_noise_lvl=1.5,\n",
    "                ):\n",
    "        self.num_cards = num_cards\n",
    "        self.win = self.num_cards//2\n",
    "        self.num_detectors = num_detectors\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.path = path\n",
    "        self.length = 0\n",
    "        self.idx_map = []\n",
    "        self.elem_rms = []\n",
    "        self.coll = glob.glob(self.path + \"/*.pickle\")\n",
    "        self.map_dataset_params()\n",
    "        self.path_noise = path_noise\n",
    "        self.coll_noise = None\n",
    "        self.length_noise = 0\n",
    "        self.max_noise_lvl = max_noise_lvl\n",
    "        self.min_noise_lvl = min_noise_lvl\n",
    "        if self.path_noise is not None:\n",
    "            self.coll_noise = glob.glob(self.path_noise + \"/*.npz\")\n",
    "            self.length_noise = len(self.coll_noise)\n",
    "        \n",
    "        \n",
    "    def map_dataset_params(self):\n",
    "        l = 0\n",
    "        for j, elem in enumerate(tqdm(self.coll)):\n",
    "            handle = open(elem, 'rb')\n",
    "            a_dict = pickle.load(handle)\n",
    "            handle.close()\n",
    "            for i in range(len(a_dict[\"data\"])):\n",
    "                self.idx_map.append([j, i])\n",
    "                self.elem_rms.append(np.sqrt(np.mean(a_dict[\"data\"][i]**2)))\n",
    "            l += len(a_dict[\"data\"])\n",
    "        self.length = l\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0: # Допилить для win > 3\n",
    "            out = np.zeros((self.num_cards, 2), dtype='int')-999\n",
    "            out[1:, :] = np.array(self.idx_map[idx:idx+self.win+1])\n",
    "        elif idx == self.length: # Допилить для win > 3\n",
    "            out = np.zeros((self.num_cards, 2), dtype='int')-999\n",
    "            out[:-1, :] = np.array(self.idx_map[idx-self.win:idx+self.win+1])\n",
    "        else:\n",
    "            out = self.idx_map[idx-self.win:idx+self.win+1]\n",
    "            out = np.array(out)\n",
    "        elems = out[:,0]\n",
    "        cards = out[:,1]\n",
    "        mask = elems == elems[self.win]\n",
    "        handle = open(self.coll[elems[self.win]], 'rb')\n",
    "        a_dict = pickle.load(handle)\n",
    "        handle.close()\n",
    "        xs = [a_dict[\"data\"][idx] for idx in cards[mask]]\n",
    "        y = a_dict[\"targets\"][cards[self.win]]\n",
    "        out = self.transform(xs, y, mask)\n",
    "        if self.path_noise is not None:\n",
    "            scalar = (self.min_noise_lvl+random.random()*(self.max_noise_lvl-self.min_noise_lvl))*self.elem_rms[idx]\n",
    "            return out[0]+ scalar*self.pick_random_noise(), out[1]\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "    \n",
    "    def pick_random_noise(self):\n",
    "        idx = random.randint(1, self.length_noise-2)\n",
    "        idx = [idx-1, idx, idx+1]\n",
    "        noises = []\n",
    "        roll = np.random.randint(0, self.num_detectors)\n",
    "        for i in idx:\n",
    "            buff = np.load(self.coll_noise[i])[\"noise\"].squeeze()\n",
    "            buff = buff/np.max(np.abs(buff))\n",
    "            buff = np.roll(buff, roll, axis=0)\n",
    "            noises.append(buff)\n",
    "        return torch.tensor(np.array(noises), dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    def transform(self, xs, y, mask):\n",
    "        xr = np.zeros((self.num_cards, self.num_detectors, self.num_timesteps))\n",
    "        indicies = mask.nonzero()[0]\n",
    "        sub_d = np.round(np.linspace(0, xs[self.win].shape[1]-1, num=self.num_detectors)).astype(int)\n",
    "        sub_t = np.round(np.linspace(0, xs[self.win].shape[0]-1, num=self.num_timesteps)).astype(int)\n",
    "        for i in indicies:\n",
    "            buff = xs[i-indicies[0]].T\n",
    "            buff = buff[:, sub_t]\n",
    "            buff = buff[sub_d, :]\n",
    "            xr[i, :, :] = buff\n",
    "        \n",
    "        y = y*(self.num_timesteps/xs[self.win].shape[0])\n",
    "        y = y[sub_d]\n",
    "        return torch.tensor(xr, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    \n",
    "    def map_rms_lvl(self):\n",
    "        l = 0\n",
    "        for j, elem in enumerate(self.coll):\n",
    "            handle = open(elem, 'rb')\n",
    "            a_dict = pickle.load(handle)\n",
    "            handle.close()\n",
    "            for i in range(len(a_dict[\"data\"])):\n",
    "                self.elem_rms.append(np.sqrt(np.mean(a_dict[\"data\"][i]**2)))\n",
    "            l += len(a_dict[\"data\"])\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886cdec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = GeoDataset(NUM_CARDS, NUM_DETECTORS, NUM_TIMESTEPS,\n",
    "                     path=\"/home/andrey/Elastic/Elements\",\n",
    "                     path_noise=\"/home/andrey/Noise Dataset\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac846a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d64a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f15681",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgs, targets = next(iter(dataloader))\n",
    "for i in range(imgs.shape[0]):\n",
    "    xs = imgs[i].numpy()\n",
    "    y = targets[i].numpy()\n",
    "    plot_element(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67a1bb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXtBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_channels, norm_groups=32, expansion_rate=4):\n",
    "        super().__init__()\n",
    "        self.dw_conv = nn.Conv2d(num_channels, num_channels, kernel_size=7, padding=3, groups=num_channels)\n",
    "        self.group_norm = nn.GroupNorm(norm_groups, num_channels)\n",
    "        hidden_channels = expansion_rate * num_channels\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, hidden_channels, kernel_size=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_channels, num_channels, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.dw_conv(x)\n",
    "        out = self.group_norm(out)\n",
    "        out = self.feed_forward(out)\n",
    "        x = x + out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1D(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "#         model_channels=256, # Тут я уменьшил, как обсудили в прошлый раз\n",
    "        model_channels=128,\n",
    "        num_channels=3,\n",
    "        groups=32,\n",
    "        expansion_rate=4,\n",
    "        dim_mult=(1, 2, 4, 8),\n",
    "        num_blocks=(3, 3, 3, 3),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, model_channels, kernel_size=(1, 4), stride=(1, 4)),\n",
    "            nn.GroupNorm(groups, model_channels)\n",
    "        )\n",
    "        \n",
    "        hidden_dims = list(map(lambda mult: model_channels * mult, (1,) + dim_mult))\n",
    "        in_out_dims = list(zip(hidden_dims[:-1], hidden_dims[1:]))\n",
    "        self.resnext_blocks = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                *[ResNeXtBlock(in_dim, groups, expansion_rate) for _ in range(num_block)],\n",
    "                nn.GroupNorm(groups, in_dim),\n",
    "                nn.Conv2d(in_dim, out_dim, kernel_size=(1, 2), stride=(1, 2))\n",
    "            ) for (in_dim, out_dim), num_block in zip(in_out_dims, num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.out_layer = nn.Linear(in_out_dims[-1][-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.resnext_blocks(x)\n",
    "        x = x.mean(-1).transpose(-1, -2)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37824ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet1D(num_channels=NUM_CARDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b06d55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 3, 60, 512), depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963d02a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(model, input_size=(1,3,60,512), expand_nested=True, depth=4, graph_dir=\"TD\")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc518c0f",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTrainer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.model = ResNet1D(num_channels=NUM_CARDS)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=2e-4)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#             optimizer, start_factor=0.0002, end_factor=1.0, total_iters=5000\n",
    "#         )\n",
    "        return [optimizer]\n",
    "    \n",
    "    def model_step(self, batch, stage):\n",
    "        img, target_timesteps = batch\n",
    "        pred_timesteps = self.model(img)\n",
    "        loss = self.loss(pred_timesteps, target_timesteps)\n",
    "        self.log(f'{stage}_loss', loss.detach().cpu().item())\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.model_step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.model_step(batch, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"01_augmented_test\"\n",
    "tb_logger = pl.loggers.TensorBoardLogger(name=name, save_dir=\"./tb_logs_augmented\", default_hp_metric=False)\n",
    "callbacks = [\n",
    "    pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=\"./saved_models/\"+name, filename=\"{step}\", monitor=\"train_loss\", mode=\"min\",\n",
    "        save_top_k=-1, every_n_train_steps=5000\n",
    "    )\n",
    "]\n",
    "trainer = pl.Trainer(\n",
    "    logger=tb_logger,\n",
    "    callbacks=callbacks,\n",
    "#     gpus=1,\n",
    "    log_every_n_steps=5,\n",
    "    max_steps=500000,\n",
    "    gradient_clip_val=1.0,\n",
    "    gradient_clip_algorithm=\"value\",\n",
    "    accumulate_grad_batches=4, # Тут запилить нормальные args\n",
    ")\n",
    "model = GeoTrainer(NUM_CARDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db630c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {\"./tb_logs_augmented\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b6fb0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11e6bf",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"baseline_9k_steps.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2141a5",
   "metadata": {},
   "source": [
    "imgs, gt = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef1254",
   "metadata": {},
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f2bd3",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f853aa",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_pred = model.model(imgs.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9e0de",
   "metadata": {},
   "source": [
    "def plot_element_gt(xs, gt, pr):\n",
    "    fig, ax = plt.subplots(1, xs.shape[0], figsize=(25, 5))\n",
    "    for i in range(xs.shape[0]):\n",
    "        buff = xs[i].T\n",
    "        if np.any(buff):\n",
    "            for j in range(buff.shape[1]):\n",
    "                buff[:, j] = buff[:, j]/np.max(np.abs(buff[:, j]))\n",
    "        ax[i].imshow(buff, cmap='gray', vmin=-0.05, vmax=0.05)\n",
    "        ax[i].axis('off')\n",
    "        ax[i].set_aspect('auto')\n",
    "    ax[xs.shape[0]//2].plot(gt, c='r')\n",
    "    ax[xs.shape[0]//2].plot(pr, c='b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d9872",
   "metadata": {},
   "source": [
    "for i in range(imgs.shape[0]):\n",
    "    xs = imgs[i].numpy()\n",
    "    y_gt = gt[i].numpy()\n",
    "    y_pr = y_pred[i].detach().cpu().numpy()\n",
    "    plot_element_gt(xs, y_gt, y_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63540757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
